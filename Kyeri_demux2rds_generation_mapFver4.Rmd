---
title: "Demultiplex to rds generation with labeled map file ver 4"
author: kyeri kim
output:
  html_document:
    df_print: paged
---

# Setup
## Load Libraries
```{r}
library(readr)
library(fs)
library(R.utils)
```

## Paths, Directories, and Shell Variables
To keep the code readable and portable, it is nice to assign paths to variables.  I recommend assigning all the paths that you will need to R variables, then using that the R `Sys.setenv` command to make shell variables that are accessible in the bash chunks from the R variables.

```{r}
data.dir = "/home/guest/project1_ibiem/rawdata/argonne_data"
output.dir = path.expand("/home/guest/project1_ibiem/demultiplexed2")
demux.dir = file.path(output.dir, "demux_all")
demux.our.dir = file.path(output.dir, "demux_our")
dir.create(demux.dir, recursive = TRUE)

#map.file = file.path(data.dir,"200114_McCumber_16SFW_AS_200110.txt")
#map.file = file.path(data.dir,"200114_McCumber_16SFW_AS_200110_ver2.txt")
#map.file = file.path(data.dir,"200114_McCumber_16SFW_AS_200110_ver3.txt")
map.file = file.path(data.dir,"200114_McCumber_16SFW_AS_200110_ver4_correction.txt")
barcode.fastq = file.path(data.dir,"Undetermined_S0_L001_I1_001.fastq.gz")
r1.fastq = file.path(data.dir,"Undetermined_S0_L001_R1_001.fastq.gz")
r2.fastq = file.path(data.dir,"Undetermined_S0_L001_R2_001.fastq.gz")

Sys.setenv(RAW_FASTQ_DIR = data.dir)
Sys.setenv(MAP_FILE = map.file)
Sys.setenv(OUT_DIR = output.dir) 
Sys.setenv(DEMUX_DIR = demux.dir)
Sys.setenv(BARCODE_FASTQ = barcode.fastq)
Sys.setenv(R1_FASTQ = r1.fastq)
Sys.setenv(R2_FASTQ = r2.fastq)
```


## Check Data Integrity
Run `md5sum` on the raw data to be sure there has been no data loss or corruption.  The md5 checksum file is in the same directory as the FASTQs and map file.

%%%% Kyeri: I changed map file so it gives error for that. I will skip this part. 
```{bash}
cd $RAW_FASTQ_DIR

```
md5sum -c md5_checksum_compressed_fastqs.txt

# Assemble Metadata Table (Map)

## Check  Map File
QIIME is inflexible about map file formatting.  Fortunately, QIIME includes the 
[validate_mapping_file.py](http://qiime.org/scripts/validate_mapping_file.html)
script that checks your map file to see if the format meets its specifications.  Unfortunately the script is not very robust, so incorrectly formated map files sometimes make it crash without giving a useful error message.  Let's run it anyway on the map file to be sure that the QIIME scripts won't choke on it.


```{bash}
validate_mapping_file.py -m $MAP_FILE -o $OUT_DIR/validate_mapfile
```
Once you have run `validate_mapping_file.py` you can view the report through RStudio:

  1. In the *Files* pane, Navigate to `r file.path(output.dir, "validate_mapfile")`
  2. Click on `sample_metadata.tsv.html` and select *View in Web Browser*
  3. Look around the output! How does it look?

## Examine Metadata Table (Map)
In the chunk below you should load the map file into a dataframe, then print the first six rows of the dataframe.
```{r}
meta.df = read_tsv(map.file)
head(meta.df)
```



# Demultiplexing

## Running split_libraries_fastq.py
Start off running split_libraries_fastq.py on the R1 FASTQ
```{bash}
set -u
TAGDIR=$DEMUX_DIR/tagged_4
split_libraries_fastq.py -r 999 -n 999 -q 0 -p 0.0001 \
		--sequence_read_fps $R1_FASTQ \
		--output_dir $TAGDIR \
		--barcode_read_fps $BARCODE_FASTQ \
		--mapping_fps $MAP_FILE \
		--phred_offset 33 \
		--barcode_type golay_12 \
		--rev_comp_barcode \
		--rev_comp_mapping_barcodes \
		--store_demultiplexed_fastq \
		--retain_unassigned_reads
```

Now verify that most of the reads are demultiplexed to a specific sample
```{bash}
ls $DEMUX_DIR/tagged_4/
cat $DEMUX_DIR/tagged_4/histograms.txt
cat $DEMUX_DIR/tagged_4/split_library_log.txt
```



## Running `split_sequence_file_on_sample_ids.py`
Now let's run `split_sequence_file_on_sample_ids.py` to actually do the demultiplexing

```{bash}
TAGDIR=$DEMUX_DIR/tagged_4
SPLITDIR=$DEMUX_DIR/split_4
split_sequence_file_on_sample_ids.py --input_seqs_fp $TAGDIR/seqs.fastq \
					 --file_type fastq \
					 --output_dir $SPLITDIR
					 
```

Now let's check that it worked
```{bash}
ls -lSrh $DEMUX_DIR/split_4
```
You should see FASTQs ranging in size from about 36kb to 90kb, plus a small `Unassigned.fastq`

## Putting it together for R1 and R2
Now that we have everything tested, let's put it together:

1. Run `split_libraries_fastq.py` on both R1 and R2 (We can drop "--retain_unassigned_reads" since we have already reviewed the results.)
2. Run `split_sequence_file_on_sample_ids.py` on the results of `split_libraries_fastq.py`
3. Do a little cleanup: get rid of the output of `split_libraries_fastq.py` once we have demuxed it since we don't need it anymore.
```{bash}
set -u
for CURREAD in "Undetermined_S0_L001_R1_001" "Undetermined_S0_L001_R2_001"
do
   CURREAD_DIR=$DEMUX_DIR/${CURREAD}
   TAGDIR=$CURREAD_DIR/tagged
    split_libraries_fastq.py -r 999 -n 999 -q 0 -p 0.0001 \
        --sequence_read_fps $RAW_FASTQ_DIR/${CURREAD}.fastq.gz \
        --output_dir $TAGDIR \
        --barcode_read_fps $BARCODE_FASTQ \
        --mapping_fps $MAP_FILE \
        --phred_offset 33 \
        --barcode_type golay_12 \
        --rev_comp_barcode \
        --rev_comp_mapping_barcodes \
        --store_demultiplexed_fastq 

        
    split_sequence_file_on_sample_ids.py --input_seqs_fp $TAGDIR/seqs.fastq \
                     --file_type fastq \
                     --output_dir $CURREAD_DIR
                     
    rm -rf $TAGDIR
done

```

Let's see if that worked by listing the contents of the directory that we demultiplexed into.
```{bash}
ls $DEMUX_DIR/
```

There should be an R1 directory with all the demultiplexed R1 FASTQs and an R2 directory with all the demultiplexed R2 FASTQs.  Let's check to be sure we see all the FASTQs

```{bash}
ls $DEMUX_DIR/Undetermined_S0_L001_R*
```

The demuxed R1 reads are in the `R1` directory and the demuxed reverse reads should be in the `R2` directory.  We are ready for DADA2!


## Bonus: Rename, move, and gzip the split FASTQs

```{r}
for (curread in c("Undetermined_S0_L001_R1_001", "Undetermined_S0_L001_R2_001")){
  curpath = file.path(demux.dir,curread)
  print(curpath)
  for (fastq_path in list.files(curpath, full.names = TRUE, pattern = ".fastq")){
    print(fastq_path)
    new_path = path_ext_remove(fastq_path)
    print(new_path)
    new_path = path_file(new_path)
    print(new_path)
    new_path = path(demux.dir, new_path, ext=paste0(curread,".fastq.gz"))
    print(new_path)
    gzip(fastq_path, new_path)
  }
}
```

Now check your work by listing the contents of the directory that contains the gzipped R1 and R2 FASTQs
```{r check_combined}{r}
list.files(demux.dir)
```



# Setup
## Load Libraries
```{r}
library(dada2)
library(readr)
library(stringr)
library(dplyr)
library(tibble)
library(magrittr)
library(phyloseq)
library(ggplot2)
library(fs)
```

## Paths, Directories, and Shell Variables
To keep the code readable and portable, it is nice to assign paths to variables.

```{r}
output.dir = path.expand("/home/guest/project1_ibiem/demultiplexed2")
demux.dir = file.path(path.expand("/home/guest/project1_ibiem/demultiplexed2"), "demux_all")
```

```{r}
scratch.dir = file.path(output.dir, "dada2")

data.dir = "/home/guest/project1_ibiem/rawdata/argonne_data"
map.file = file.path(data.dir,"200114_McCumber_16SFW_AS_200110_ver4_correction.txt")

# output directory
if (dir_exists(scratch.dir)) {
  dir_delete(scratch.dir)
}
dir_create(scratch.dir)

ps.rds = file.path(scratch.dir, "ibiemProject1_subset.rds")

# NOT SURE IF WE USE THIS REFERENCES FOR OUR PROJECT YET
silva.ref = "/data/references/dada/silva_nr_v132_train_set.fa.gz"
silva.species.ref = "/data/references/dada/silva_species_assignment_v132.fa.gz"
```

## Checking Files 
Let's make sure that our demultiplexed files are all where we expect them

```{r view_demuxed_files}
head(list.files(demux.dir))
```

## Filter and Trim

First we read in the names of the fastq files, and perform some string manipulation to get lists of the forward and reverse fastq files in matched order.  We also need to generate a vector of sample names from one of these lists of fastq file paths.
```{r}
fnFs <- sort(list.files(demux.dir, pattern="R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(demux.dir, pattern="R2_001.fastq", full.names = TRUE))

forward_fastq_suffix = ".Undetermined_S0_L001_R1_001.fastq.gz"

sample.names = fnFs %>% 
  basename %>%
  str_replace(forward_fastq_suffix,"") 
```

Look at the list of FASTQs to be sure it only contains R1 files
```{r}
head(print(fnFs))
head(print(fnRs))
head(print(sample.names))
```


## Examine quality profiles of forward and reverse reads

Please visualize the quality profiles of the *un-demultiplexed* reads (i.e. the full R1 and R2 files before demultiplexing).  Since the R1 and R2 files are subset from a full MiSeq run, each should have 20,000 reads.
```{r}
list.files(data.dir)
```

## Forward Read Quality Profile
```{r}
plotQualityProfile(file.path(data.dir, "Undetermined_S0_L001_R1_001.fastq.gz"))
```
## Reverse Read Quality Profile
Now we visualize the quality profile of the reverse reads:
```{r}
plotQualityProfile(file.path(data.dir, "Undetermined_S0_L001_R2_001.fastq.gz"))
```

## Perform filtering and trimming

### Generate filenames for the filtered fastq.gz files.
```{r}
#put all the filtered FASTQs in their own directory
filt_path <- file.path(scratch.dir, "filtered") 

filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
```


### Filter the forward and reverse reads
Now let's do the filtering using the parameters we chose based on the quality plots
```{r}
filt.out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=10,
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=FALSE) 
head(filt.out)
```
### Tweak Filtered FASTQ list
There are a few samples that have very few reads to start with and no reads after filtering. If a sample doesn't have any reads after filtering, `filterAndTrim` doesn't bother to make an empty filtered FASTQ.  We need to manage this, because downstream steps will give us an error if we give them a list of filtered FASTQ filenames that contains names of files that don't actually exist.  We need to regenerate our list of filtered FASTQs based on the filtered FASTQ files that are actually present in the directory. We also need to regenerate `sample.names` from the list of filtered FASTQ files.
```{r}
filtFs = filtFs[file_exists(filtFs)]
filtRs = filtRs[file_exists(filtRs)]
```

## Learn the Error Rates
With that bit of cleanup done, we are ready to build an error model from the filtered FASTQs.  We do this seperately for the R1 and R2 FASTQs, since they have different patterns of errors, as we have seen.
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```

It is always worthwhile, as a sanity check if nothing else, to visualize the estimated error rates:
```{r}
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```

The error rates for each possible transition (eg. A->C, A->G, ...) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence. The red line shows the error rates expected under the nominal definition of the Q-value. Here the black line (the estimated rates) fits the observed rates well, and the error rates drop with increased quality as expected. 

Since we are using a small subset of the data, the learned error rates might look funny.  This is OK for now, since we are just doing a pilot analysis.

## Dereplication

Dereplication combines all identical sequencing reads into into "unique sequences" with a corresponding "abundance" tally: the number of reads with that unique sequence. Dereplication substantially reduces computation time for the inference step, since we only need to do inference for unique sequences.  

```{r}
filtFs %>% 
  basename %>%
  str_replace("_F_filt.fastq.gz","") ->
  sample.names
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
```

### Rename derep objects
`derepFastq` returns a list of derep objects that are named based on the input filename.  For later steps it is going to be more convenient if each derep object is named with just it's sample name, so let's rename the derep objects using the sample name vector that we created earlier.
```{r}
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

## Sample Inference

Now you need to infer the true sequences from the dereplicated data. 

```{r}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
```
```{r}
dadaFs[[1]]

dadaRs[[1]]
```

```{r}
length(derepFs[[1]]$map)
length(derepRs[[1]]$map)
```

## Merge paired reads
Each pair of R1 and R2 reads represents one observed sequence, so we ultimately need to combine them into a single sequence.  The paired reads allows us to reduce sequence error where the reads overlap because that part of the insert has been sequenced twice so we can compare the two sequences and be sure they agree.  This is why it is desireable to have reads that overlap as much as possible.

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
head(mergers[[2]])
```
The results of `mergePairs` is a list of data frames with one data frame for each sample.  Each data frame contains the unique merged sequences observed for that sample along with information about each sequence.

Paired reads that did not perfectly match in the overlapping region were removed by `mergePairs`.


## Construct sequence table

We can now construct a sequence table of our samples.  This the equivalent of the OTU table produced by other methods.  The sequence table has a row for each sample and a column for each ASV (the DADA2 equivalent of an OTU), with the count of the number of each ASV observed in each sample.
```{r}
seqtab <- makeSequenceTable(mergers)
```


Let's check the dimensions of the sequence table.  How many samples are there?  How many ASVs?
```{r}
dim(seqtab)
```

Let's check the size distribution of the ASVs we have inferred. 
```{r}
table(nchar(getSequences(seqtab)))
```
 In most bacteria the amplicon is 253bp, but there is some variation in the length of the V4 region, so we expect some amplicons to be a few bp shorter and longer.  *Note* the ASVs will be shorter by the total amount that you trimmed from the left (5') of the reads, so if you trimmed 5bp from the left of the R1 reads and 7bp from the left of the R2 reads, you expect the amplicons to be about 253bp - 5bp - 7bp = 241bp.
 
## Remove chimeras

The core `dada` method removes substitution and indel errors, but chimeras remain. Fortunately, the accuracy of the sequences after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as a bimera (two-parent chimera) from more abundant sequences.  Let's remove chimeric sequences

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim)/sum(seqtab)
```


## Track reads through the pipeline

As a final check let's make a table showing how many reads remain after each step in the pipeline for each sample.


```{r}
getN <- function(x) sum(getUniques(x))
filt.out %>%
  as_tibble(rownames = "filename") %>%
  mutate(sample=str_replace(filename, forward_fastq_suffix,"")) %>%
  select(sample, input=reads.in, filtered=reads.out) ->
  track

sapply(dadaFs, getN) %>%
  enframe(name="sample", value="denoised") ->
  denoised
track %<>% full_join(denoised, by=c("sample"))

sapply(mergers, getN) %>%
  enframe(name="sample", value="merged") ->
  merged
track %<>% full_join(merged, by=c("sample"))

rowSums(seqtab) %>%
  enframe(name="sample", value="tabled") ->
  tabled
track %<>% full_join(tabled, by=c("sample"))

rowSums(seqtab.nochim) %>%
  enframe(name="sample", value="nonchim") ->
  nonchim
track %<>% full_join(nonchim, by=c("sample"))

track
```


This is a great place to do a last **sanity check**. Outside of filtering (depending on how stringent you want to be) there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the `truncLen` parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads failed to pass the chimera check, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.</div>


## Assign taxonomy

Now you can assign taxonomy!  You should use this taxonomy reference files : `/data/references/dada/silva_nr_v132_train_set.fa.gz`
```{r}
silva.ref
```
```{r}
taxa <- assignTaxonomy(seqtab.nochim, silva.ref, multithread=TRUE)
taxa <- addSpecies(taxa, silva.species.ref)
taxa.print <- taxa 
```
```{r}
#rownames(taxa.print) <- NULL
head(taxa.print)
```

# Phyloseq
We are now done we the DADA2 pipeline.  Let's put the results into a phyloseq object and save it to an RDS for safe keeping!

## Map Data
First we need to load the map data using phyloseq's `sample_data()` function. `sample_data()` expects the sample identifiers to be rownames, but our map file has them as a column named "#SampleID", so we need to use a function called `column_to_rownames` to convert this column into rownames


```{r}
meta.df = read_tsv(map.file, comment= "#q2") %>%
  rename(Sample = "#SampleID") %>%
  column_to_rownames("Sample") %>%
  as.data.frame
```
```{r}
head(meta.df)
```

## Make a Phyloseq Object
Now we can construct a phyloseq object directly from the dada2 outputs and the map data frame.
```{r}
otus = otu_table(seqtab.nochim, taxa_are_rows=FALSE)

sd = sample_data(meta.df)   # sample_data doesn't work in here

ps <- phyloseq(otus,
               sd,
               tax_table(taxa))
```

And `print` your phyloseq object to be sure it was created correctly
```{r}
print(ps)
```

Your results from the previous chunk should look like this (number of taxa could be different depending on parameter choices): 
```
phyloseq-class experiment-level object
otu_table()   OTU Table:         [ 234 taxa and 107 samples ]
sample_data() Sample Data:       [ 107 samples by 17 sample variables ]
tax_table()   Taxonomy Table:    [ 234 taxa by 6 taxonomic ranks ]
```

## Save Phyloseq to RDS
Any R object can be saved to an RDS file.  It is a good idea to do this for any object that is time consuming to generate and is reasonably small in size.  Even when the object was generated reproducibly, it can be frustrating to wait minutes or hours to regenerate when you are ready to perform downstream analyses.

We will do this for our phyloseq object to a file since it is quite small (especially compared to the size of the input FASTQ files), and there were several time consuming computational steps required to generate it.  
```{r}
write_rds(ps, ps.rds)
loaded.ps = read_rds(ps.rds)
```

## Check Phyloseq RDS

We can now confirm that it worked.  Load your phyloseq RDS and `print` it. The results should be the same as above.
```{r}
print(loaded.ps)
```


# Session Info
Always print `sessionInfo` for reproducibility!
```{r}
sessionInfo()
```
